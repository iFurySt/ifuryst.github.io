---
layout: post
title: "从Manus构建AI Agent看上下文工程"
date: 2025-07-21T00:00:00+00:00
tags:
  - AI
  - Tech
categories: AI
giscus_comments: true
tabs: true
pretty_table: true
toc:
  sidebar: left
---

今天读到Manus的一篇分享：[**Context Engineering for AI Agents: Lessons from Building Manus**](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)**。**内容是关于如何构建他们的 AI Agents，特别是其中关于**上下文工程（Context Engineering）**的内容。这篇文章干货非常多，虽然有些表述偏专业，但值得认真阅读。我会抽取其中一些关键内容结合我的想法聊聊

## 上下文工程是一门实验科学

文章提出一个非常核心的观点：上下文工程是一门**实验科学（It's an experimental science）**。这正是我之前也提到过的观点。在大语言模型的时代，是没有标准答案的。一个简单的Prompt，可以写出成千上万种形式，没有唯一解。就像写作文一样，文无第一武无第二。适合你业务和应用场景的才是最好的。

## 为什么上下文工程如此重要？

AI Agents 在运行时本质上是无状态的，每一次调用都是独立的。它能否执行好任务，完全取决于我们给它的**上下文**。换句话说，它是否知道当前处于哪个步骤，面对什么问题，依赖的就是我们构建的上下文

而在 Agent 的推理过程中，真正的“记忆”并不在模型的权重里，而是靠我们在每次调用时提供的上下文构建而成。因此，上下文是 AI 运行中最接近“记忆”的存在，其重要性不言而喻。

## 缓存命中与前缀稳定

文章中提到一个非常重要的技术细节：**命中换存（KV-cache hit rate）**，尤其是在大模型推理中对于 token 的处理

如果你每次调用时前缀内容完全一致（比如固定的System Prompt、角色设定、工具列表等），那么这部分可以命中缓存，大幅降低 token 推理成本（比如Sonnet 4的差价可达 10 倍）

关键在于：**前缀越稳定、越固定，就越有可能命中缓存，从而降低成本**。这也是 Manus 在构建 AI Agents 时非常强调的一点

我们其实可以看到现在很多AI Agent的System Prompt都非常的长！其实他们都是不变的，所以都是命中缓存，这个消耗是非常之小的（Claude Sonnet 4命中缓存和不命中能相差10倍的费用），所以如果有这个认知的话，对于设计AI Agent或者其他相关的应用时就可以心里有底了

## 分块处理：上下文过长的解决方案

我们经常会遇到这样的情况：需要翻译或总结一篇很长的文章，直接一股脑塞进 prompt，模型虽然能吃下去，但：

- 要么窗口装不下
- 要么吃进去效果反而更差，因为信息太杂、注意力被分散

Manus 提到了使用的方法：**分段处理 + 汇总再整合**。比如翻译任务时，他们会将一整篇文章切成多个文件，然后分文件进行翻译并输出到多个文件。最终只需要把翻译结果整合返回给用户即可

平时用一个AI Coding Agent的时候其实也能看到，一般会读取类似1-100行、101-200行这样的内容，其实也是变相的在做这样的控制，有时候单行太长就会有问题，不过有一些AI Agent会适应成根据字符长度拉取内容

## 关于 Few-Shot：使用要谨慎

我们经常说少样本（few-shot）示例能提高模型模仿能力，但 Manus 的经验提醒我们：**Few-Shot 是一把双刃剑。**

尤其当你把模型先前的响应结果一起带入到新一轮推理中时，就可能造成结果偏差。模型会误以为“你希望我继续往这个方向走”，久而久之形成错误的趋势。

所以将 few-shot 的使用控制在较稳定的 system prompt 中，比如说明工具调用格式、响应格式等，而不要过多在用户历史上下文中混入大量示例。这可以降低信息污染的风险，保持模型判断的中立性

## 如何保持任务专注：To-Do List

对于基于ReAct这类任务型Agent来说，初始会进行Plan并拆解出一个 To-Do List，指示执行顺序和目标

但随着上下文膨胀，To-Do List 可能会被其他上下文淹没，导致模型无法持续聚焦于目标，可能导致无法顺利完成或者以较好的效果完成

Manus 的做法是：**在任务开始和每个步骤结束时反复更新 to-do list，提醒模型当前的目标和进度**。这样模型每次都能重新聚焦，避免走远。

这个小技巧虽然简单，但非常有效，特别适合构建需要持久执行逻辑的场景。我们也可以在Claude Code等Agent里看到类似的应用

## 总结：上下文工程是 AI 工程化的核心

从 Manus 的这篇文章中可以看到，虽然它是在讲述 AI Agents 的实现，但真正的核心仍是**上下文工程**

我们在做 AI 应用、做 RAG、做 Agent、做工具调用时，其实最核心的技术壁垒就是如何构建、维护、压缩、动态更新上下文

从系统提示（System Prompt）、工具说明、历史记录，到对话摘要、外部搜索、记忆管理，都是为了解决一个问题——**给模型一个正确的、足够的信息场**，让它能做出准确决策

所以说，除了大语言模型本身的能力，**上下文工程是决定你 AI 能不能跑得好、跑得稳的第二曲线技术核心**
