<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 大模型上下文工程实践指南-第5章：检索增强生成 | ifuryst </title> <meta name="author" content="Leo Li"> <meta name="description" content="📝 &amp; 💭 "> <meta name="keywords" content="📝 💭 🆒 🏝️ 💻"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%95%B6%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ifuryst.github.io/blog/2025/ce101-5-rag/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> ifuryst </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">关于 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">文章 </a> </li> <li class="nav-item "> <a class="nav-link" href="/life/">生活 </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> <li class="nav-item"> <a class="nav-link" href="https://en.ifuryst.com/" rel="external nofollow noopener" target="_blank">EN</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-3 mt-md-4 mt-lg-5 mt-xl-5 mt-xxl-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">大模型上下文工程实践指南-第5章：检索增强生成</h1> <p class="post-meta"> Created in October 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/blog"> <i class="fa-solid fa-hashtag fa-sm"></i> Blog</a>   <a href="/blog/tag/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7"> <i class="fa-solid fa-hashtag fa-sm"></i> 微信公众号</a>   <a href="/blog/tag/substack"> <i class="fa-solid fa-hashtag fa-sm"></i> Substack</a>   ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> Blog</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="61-rag基础与原理">6.1 RAG基础与原理</h1> <h2 id="611-rag基础概念">6.1.1 RAG基础概念</h2> <p>检索增强生成（RAG，Retrieval-Augmented Generation）是由Facebook（现Meta） AI Research在2020年的一篇<a href="https://arxiv.org/abs/2005.11401" rel="external nofollow noopener" target="_blank">论文</a>中出的一个技术，提出的原因是大语言模型（LLM）虽然在各种任务上表现优异，但由于<strong>知识存储在参数中</strong>，<strong>无法及时更新且易出现幻觉（Hallucination）</strong>；因此引入外部可检索的非参数化记忆，并将检索结果与模型结合，从而提升知识密集型任务的准确性与可追溯性。</p> <p>简单的人话表述就是，大模型需要外部的信息来帮助决策，提前将文档通过一些手段（分块、向量化等）存起来后，查询的时候可以在这些内容中搜索辅助大模型进行最终的回答，整个流程下来就是RAG要做的一个事情。</p> <p>RAG能流行是因为其解决了这么几个问题：</p> <ul> <li> <strong>解决推理使用的是过时的训练语料库</strong>：尤其针对一些对时间较为敏感的数据，以及一些个人/企业知识库需要最新的</li> <li> <strong>缓解幻觉（Hallucination）</strong>：RAG可以极强的缓解幻觉，这个核心还是因为模型基于上下文进行推理的过程可以产生更加可靠的结果</li> <li> <strong>通用模型专业化</strong>：尤其针对垂直领域时，通用模型权重过于分散，在搭配该领域的知识库后，可以有效提升专业化，提高结果的可靠性</li> </ul> <p>我们采用Langchain官方这个<a href="https://python.langchain.com/docs/tutorials/rag/" rel="external nofollow noopener" target="_blank">教程</a>里的图演示RAG是怎么运作的：</p> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-10-01-ce101-5-rag/1759293252_1-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293252_1-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293252_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-10-01-ce101-5-rag/1759293252_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </div> <p>文档通过这个流程进行分块、向量化和存储。然后到查询环节：</p> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-10-01-ce101-5-rag/1759293252_2-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293252_2-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293252_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-10-01-ce101-5-rag/1759293252_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </div> <p>召回Top K的结果，结合提示词给到大模型做最后的输出。下面是一个简单的Demo：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span><span class="p">,</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">from</span> <span class="n">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>

<span class="c1"># 配置日志格式
</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">basicConfig</span><span class="p">(</span>
<span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="p">.</span><span class="n">INFO</span><span class="p">,</span>
<span class="nb">format</span><span class="o">=</span><span class="sh">"</span><span class="s">%(asctime)s [%(levelname)s] %(message)s</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Step 1: 准备文档
</span>
<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
<span class="sh">"</span><span class="s">Leo 发明了一种新的编程语言，名字叫做 CatLang。</span><span class="sh">"</span><span class="p">,</span>
<span class="sh">"</span><span class="s">CatLang 的语法非常简单，所有函数都以 </span><span class="sh">'</span><span class="s">喵</span><span class="sh">'</span><span class="s"> 开头。</span><span class="sh">"</span><span class="p">,</span>
<span class="sh">"</span><span class="s">在 2025 年，Leo 还发布了一个框架叫做 PurrNet，用于分布式 AI 计算。</span><span class="sh">"</span><span class="p">,</span>
<span class="sh">"</span><span class="s">PurrNet 的核心是通过小猫节点来进行任务调度，每个节点代号是 Kitten。</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">准备文档完成，共 %d 条</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">))</span>

<span class="c1"># Step 2: 文本切分（可选）
</span>
<span class="n">splitter</span> <span class="o">=</span> <span class="nc">CharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">.</span><span class="nf">split_text</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">texts</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">文档切分: 原文=%s -&gt; %d 个切片</span><span class="sh">"</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">))</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">所有切分后的文本总数: %d</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">texts</span><span class="p">))</span>

<span class="c1"># Step 3: 向量化 &amp; 建立向量数据库
</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">text-embedding-3-small</span><span class="sh">"</span><span class="p">)</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">开始向量化...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">from_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">向量数据库建立完成</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Step 4: 构建 RAG QA Chain
</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">similarity</span><span class="sh">"</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">)</span>
<span class="n">qa</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">RAG QA Chain 构建完成</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Step 5: 提问
</span>
<span class="n">query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">什么是CatLang？</span><span class="sh">"</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">开始提问: %s</span><span class="sh">"</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">qa</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="c1"># 检索过程可视化（教学用）
</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">检索到的相关文档（Top 2）:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">get_relevant_documents</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">retrieved_docs</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">文档 %d: %s</span><span class="sh">"</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">====== 最终结果 ======</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">问题:</span><span class="sh">"</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">回答:</span><span class="sh">"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=====================</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>这是一个很简单的例子，我随便虚构了一些大模型不可能“知道”的内容，这样可以避免大模型作弊，然后写死了，运行后输出如下：</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2025-09-21 22:25:21,127 <span class="o">[</span>INFO] 准备文档完成，共 4 条
2025-09-21 22:25:21,127 <span class="o">[</span>INFO] 文档切分: 原文<span class="o">=</span>Leo 发明了一种新的编程语言，名字叫做 CatLang。 -&gt; 1 个切片
2025-09-21 22:25:21,127 <span class="o">[</span>INFO] 文档切分: 原文<span class="o">=</span>CatLang 的语法非常简单，所有函数都以 <span class="s1">'喵'</span> 开头。 -&gt; 1 个切片
2025-09-21 22:25:21,127 <span class="o">[</span>INFO] 文档切分: 原文<span class="o">=</span>在 2025 年，Leo 还发布了一个框架叫做 PurrNet，用于分布式 AI 计算。 -&gt; 1 个切片
2025-09-21 22:25:21,127 <span class="o">[</span>INFO] 文档切分: 原文<span class="o">=</span>PurrNet 的核心是通过小猫节点来进行任务调度，每个节点代号是 Kitten。 -&gt; 1 个切片
2025-09-21 22:25:21,127 <span class="o">[</span>INFO] 所有切分后的文本总数: 4
2025-09-21 22:25:21,335 <span class="o">[</span>INFO] 开始向量化...
2025-09-21 22:25:23,180 <span class="o">[</span>INFO] HTTP Request: POST https://api.openai.com/v1/embeddings <span class="s2">"HTTP/1.1 200 OK"</span>
2025-09-21 22:25:23,230 <span class="o">[</span>INFO] Loading faiss.
2025-09-21 22:25:23,279 <span class="o">[</span>INFO] Successfully loaded faiss.
2025-09-21 22:25:23,285 <span class="o">[</span>INFO] 向量数据库建立完成
2025-09-21 22:25:23,388 <span class="o">[</span>INFO] RAG QA Chain 构建完成
2025-09-21 22:25:23,388 <span class="o">[</span>INFO] 开始提问: 什么是CatLang？
2025-09-21 22:25:24,608 <span class="o">[</span>INFO] HTTP Request: POST https://api.openai.com/v1/embeddings <span class="s2">"HTTP/1.1 200 OK"</span>
2025-09-21 22:25:27,366 <span class="o">[</span>INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions <span class="s2">"HTTP/1.1 200 OK"</span>

2025-09-21 22:25:27,392 <span class="o">[</span>INFO] 检索到的相关文档（Top 2）:
2025-09-21 22:25:29,062 <span class="o">[</span>INFO] HTTP Request: POST https://api.openai.com/v1/embeddings <span class="s2">"HTTP/1.1 200 OK"</span>
2025-09-21 22:25:29,064 <span class="o">[</span>INFO] 文档 1: Leo 发明了一种新的编程语言，名字叫做 CatLang。
2025-09-21 22:25:29,065 <span class="o">[</span>INFO] 文档 2: CatLang 的语法非常简单，所有函数都以 <span class="s1">'喵'</span> 开头。

<span class="o">======</span> 最终结果 <span class="o">======</span>
问题: 什么是CatLang？
回答: CatLang是一种由Leo发明的新编程语言，其语法非常简单，所有函数都以“喵”开头。
<span class="o">=====================</span>
</code></pre></div></div> <p>这边我做了一个Top K搜索的模拟，实际上是不会打印的，这个简单的Demo让我们对RAG有一个初步的概念。总体而言，RAG是为了提高效果的技术，其结合文档检索，提供了合适模型的上下文，成为上下文工程中的核心技术之一。接下去我们来看一下RAG的基础架构和流程</p> <h2 id="612-架构与工作流程">6.1.2 架构与工作流程</h2> <p>接下去我们来看看RAG相关的架构和流程，这边我画了一张RAG架构图：</p> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-10-01-ce101-5-rag/1759293252_3-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293252_3-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293252_3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-10-01-ce101-5-rag/1759293252_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </div> <p>这是一个比较完整的RAG架构图，包含了流程中的一些关键节点，我们不需要马上理解每个环节，后面我们会陆续提到每个环节里的内容。</p> <p>RAG的基础架构相对简单，主要分为三个阶段：</p> <ol> <li> <strong>查询（Query）</strong>：输入，通常为用户的查询或者问题等</li> <li> <strong>检索（Retriever）</strong>：从相关知识库中获得与用户问题相关性最高的文档（Top K）</li> <li> <strong>生成（Generation）</strong>：根据Query和检索得到的文档，生成高质量的回答</li> </ol> <p>下面是一个RAG实施的全过程：</p> <ol> <li>数据通过合理的分块（chunking），每块分别做向量化（embedding）后存到向量数据库</li> <li>查询进来后，将查询问题也通过同样的方式向量化后，去到向量数据库内做相似性搜索</li> <li>将搜索得到的top-k文档块的原始数据拼接后放在上下文中一起发送给大语言模型</li> <li>大语言模型基于响应的数据做最后的结果生成</li> </ol> <p>这样有了原始数据的参考，大模型就有了参照物，最终给出的答案也会更加稳定，避免自由发挥情况下容易产生幻觉或产生过时数据的情况发生。在开始深入RAG之前，我们可以先来了解一下检索方式，这有助于我们理解RAG里一个很核心的概念，检索。</p> <h2 id="613-检索方式">6.1.3 检索方式</h2> <p>在自然语言处理中有文本检索技术，分为：</p> <ol> <li>稀疏文本检索（Sparse Retrieval）</li> <li>稠密文本检索（Dense Retrieval）</li> </ol> <p>在现行的RAG语境下，更多是使用了向量化搜索，也就是稠密文本检索的方式。但是随着RAG应用的推广和普及，目前越来越多应用中会将两个检索方式结合起来使用，这个在下一节中也会了解到。现在我们先来了解一下这两种检索方式的原理和差异。</p> <h3 id="稀疏文本检索sparse-retrieval">稀疏文本检索（Sparse Retrieval）</h3> <p>原理是<strong>基于词频（Term Frequency）等显式词项统计信息，使用稀疏向量（Sparse Vector）表示文本，使用向量相似度进行匹配，返回最相关的文档</strong>。那么什么是稀疏向量呢？简单说就是大部分维度为0的向量。简单举个例子来理解，假设有个词表（vocabulary）：</p> <p>```plain text [“apple”, “banana”, “car”, “dog”, “elephant”]</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
这个词表有5个词，对应一个5维的向量空间。现在有个文档：

```plain text
I like banana
</code></pre></div></div> <p>我们用稀疏向量来表示这个文档时，会得到：</p> <p>```plain text [0, 1, 0, 0, 0]</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
很直观的可以看到，这是一个5维的向量，但是其中大部分的维度都是0（没出现），只有极少数是非0（有出现的词）。理论上我们会在这里持续增加词出现的频次，比如

```plain text
banana banana banana!
</code></pre></div></div> <p>可以得到</p> <p>```plain text [0, 3, 0, 0, 0]</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
看着没什么问题，但是这种极致简单的词频统计，会在某些情况下有问题，比如像“the”、“is”、“you”这些词在所有文本中都很多，但它们没啥实际意义。所以出现次数多的词，并不一定重要。为了解决这个问题，我们就需要引入一些方法。常见的方法有：

- **TF-IDF（Term Frequency - Inverse Document Frequency）**：在词频的基础上加入“逆文档频率”因素，降低常见词的权重，提高稀有词的权重。
- **BM25**：一种改进的 TF-IDF 加权方案，同时考虑了词频饱和、文档长度归一化等因素，广泛应用于现代搜索引擎。

这些方法都基于**倒排索引（Inverted Index）**结构实现高效检索。它们不再简单依赖“词频越高越重要”的假设，而是引入更多统计规律，使得检索系统能更准确地评估“哪些词更关键”。这个也是传统的搜索引擎的基础，像Google这类搜索引擎在早期就应用了这类技术去做搜索。另外全文检索里可以经常看到这两个技术，比如ES的全文检索就是利用了BM25来做的。

可以看出**稀疏文本检索的优点就是高效快速，消耗资源少，因此被广泛使用**。其**缺点就是无法理解一些语义相近但是词不重叠的文本**，比如car和automobile这种，因此也就有了稠密文本检索来解决这个问题

### 稠密文本检索（Dense Retrieval）

原理是**通过神经网络（如Word2Vec、BERT）将查询和文档分别编码成低维稠密向量（Dense Vector），使用向量相似度（如内积或余弦相似度）进行匹配，返回最相关的文档**。那么什么是稠密向量呢？和稀疏向量刚好反过来了：稠密向量是所有维度基本都有值的向量。每一维都用浮点数表示，通常没有“0”或者很少有“0”。

这边的低维是相对于前面稀疏文本里的稀疏向量通常是极高维度的，因为那边的向量维度=词表大小，通常可以词表可以达到**几十万甚至百万维**，但是在稠密向量里，通常**几十维到几千维**的程度，所以是低维稠密向量。

举个例子，还是前面这句话：

```plain text
I love bananas

</code></pre></div></div> <p>我们将其送进一个神经网络模型（如BERT、DPR编码器），可以输出得到一个向量，如：</p> <p>```plain text [0.12, -0.08, 0.91, 0.33, …, 0.04] // 共768维</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
像现在流行的Embedding本质上就是这个原理，通过预训练语言模型后，可以通过模型将内容编码为向量，每个向量都是一个**语义表示（Semantic Representation）**，这些向量不是手动构造的，而是模型通过大量文本学习出来的。

我们可以找到很多这种向量可视化的网站或者开源项目，比如[tensorflow](https://projector.tensorflow.org/)这个展示了word2vec的向量在三维空间的表示，可以看两个词的可视化距离（相似度计算其实算的就是在对应维度空间下的两点之间的距离，只不过维度高到人类大脑无法轻易想象，也就是超越人类的认知，没办法像在二维和三维空间下可以轻松计算距离）

&lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        &lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        

&lt;figure
  
&gt;
  &lt;picture&gt;
    &lt;!-- Auto scaling with imagemagick --&gt;
    &lt;!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    --&gt;
    
      &lt;source
        class="responsive-img-srcset"
        srcset="/assets/img/2025-10-01-ce101-5-rag/1759293254_4-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293254_4-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293254_4-1400.webp 1400w,"
        
          sizes="95vw"
        
        type="image/webp"
      &gt;
    
    &lt;img
      src="/assets/img/2025-10-01-ce101-5-rag/1759293254_4.png"
      
        class="img-fluid rounded z-depth-1"
      
      
        width="100%"
      
      
        height="auto"
      
      
      
      
      
        data-zoomable
      
      
        loading="eager"
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    &gt;
  &lt;/picture&gt;

  
&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
另外vectosphere这个，也可以同样可视化展示：
&lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        &lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        

&lt;figure
  
&gt;
  &lt;picture&gt;
    &lt;!-- Auto scaling with imagemagick --&gt;
    &lt;!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    --&gt;
    
      &lt;source
        class="responsive-img-srcset"
        srcset="/assets/img/2025-10-01-ce101-5-rag/1759293254_5-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293254_5-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293254_5-1400.webp 1400w,"
        
          sizes="95vw"
        
        type="image/webp"
      &gt;
    
    &lt;img
      src="/assets/img/2025-10-01-ce101-5-rag/1759293254_5.png"
      
        class="img-fluid rounded z-depth-1"
      
      
        width="100%"
      
      
        height="auto"
      
      
      
      
      
        data-zoomable
      
      
        loading="eager"
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    &gt;
  &lt;/picture&gt;

  
&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
回过头来，常见的稠密文本检索方法有下面这些，有兴趣的可以自己去了解一下：

我们平时最常见的RAG应用就是使用了Bi-Encoder，因为足够快，而ReRank时数量较少，可以利用Cross-Encoder来打分。

到这里我们已经知道了稠密文本检索到底是做什么了，在提前向量化资料后，在后续问题来了之后可以将问题也进行向量化，然后通过向量相似度进行搜索，得到最相关的资料，这就是稠密文本检索的过程，**能够检索语义相近但词不匹配的文档**，并且**适合复杂查询、开放域问答、RAG 等应用**。

其缺点也相对明显：**需要大规模训练，消耗资源大，部署成本高，另外召回的结果可解释性低**

### 融合方法（Hybrid Retrieval）

两者各有优缺点，因此很多系统或者应用场景会将两者进行结合，比如用稀疏检索（如BM25）结合稠密检索先召回 Top K文档，再用重排模型（Dense Reranker，如Cross-Encoder）对结果进行重新排序，重新排序

引用一张我之前发的关于Bi-Encoder和Cross-Encoder：

&lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        &lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        

&lt;figure
  
&gt;
  &lt;picture&gt;
    &lt;!-- Auto scaling with imagemagick --&gt;
    &lt;!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    --&gt;
    
      &lt;source
        class="responsive-img-srcset"
        srcset="/assets/img/2025-10-01-ce101-5-rag/1759293254_6-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293254_6-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293254_6-1400.webp 1400w,"
        
          sizes="95vw"
        
        type="image/webp"
      &gt;
    
    &lt;img
      src="/assets/img/2025-10-01-ce101-5-rag/1759293254_6.png"
      
        class="img-fluid rounded z-depth-1"
      
      
        width="100%"
      
      
        height="auto"
      
      
      
      
      
        data-zoomable
      
      
        loading="eager"
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    &gt;
  &lt;/picture&gt;

  
&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
我们在实际应用中**不会因为技术而技术**，一定要记住这句话！否则很容易陷入拿着锤子找钉子的尴尬境地（现在其实有不少人就是拿着AI找钉子敲）。就比如前面提到的这些，有可能在实际的应用中只是简单的应用向量化去做检索就足够了，也可能复杂到需要结合关系型数据库做常规的数据检索+ES做全文检索+向量化检索+重排技术得到最匹配的结果去做方案。所以应用AI（Applied AI）的背后就是我们需要去了解每个技术背后的原理，是基于什么背景之下提出来的，以及这个技术目前发展到什么程度了，可以解决什么问题，在某个应用场景下是否合适，这样我们才可以真正做到将AI应用在有价值的地方，赋能业务产生真正的商业价值，而不是陷入技术自嗨中。

了解完这个我们对于RAG的底层依托的技术已经有了比较清晰的认知了，接下去我们会进一步深入去了解RAG相关的技术以及衍生的一些应用方式。

# 6.2 RAG进阶

常规的RAG相对简单，在实际应用中，我们会在原本的架构之上，去运用一些技术和方法来提高，比如：

- **标量+向量**：通常RAG是将文档分块（Chunk）后向量化（Embedding）入库，然后查询也向量化后到向量数据库进行相似性搜索。如前面提到，实际上还可以结合传统的数据库或者ES进行标量数据的匹配检索，最后可以得到标量+向量数据。
- **重排（Reranking）**：不管是单向量还是结合了标量，在送到模型前可以用一些手段对文档进行重新排序，通常我们会使用重排模型对文档再进行评分排序，这样可以选择实际送到模型的文档
- **多跳RAG**：当单跳查询无法满足复杂的查询时，结合多跳是可以达到更好的效果的。
- **图增强RAG（Graph-RAG）**：结合图的能力来扩展RAG的能力，尤其是在文档处理阶段，可以利用图+大模型来细化一些实体和关系，甚至进一步形成社区或领域的形态。

上面只是一部分技术或方法。在技术普及过程，开始会陆续出现体系化的知识，也是为了方便应用以及后来者学习，现在业界也有很多划分方式，比如[Daily Dose of Data Science](https://www.dailydoseofds.com/tag/rag-crash-course/)这张图：

&lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        &lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        

&lt;figure
  
&gt;
  &lt;picture&gt;
    &lt;!-- Auto scaling with imagemagick --&gt;
    &lt;!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    --&gt;
    
      &lt;source
        class="responsive-img-srcset"
        srcset="/assets/img/2025-10-01-ce101-5-rag/1759293255_7-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293255_7-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293255_7-1400.webp 1400w,"
        
          sizes="95vw"
        
        type="image/webp"
      &gt;
    
    &lt;img
      src="/assets/img/2025-10-01-ce101-5-rag/1759293255_7.jpg"
      
        class="img-fluid rounded z-depth-1"
      
      
        width="100%"
      
      
        height="auto"
      
      
      
      
      
        data-zoomable
      
      
        loading="eager"
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    &gt;
  &lt;/picture&gt;

  
&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
另外[这篇论文](https://arxiv.org/pdf/2501.09136)里也提供了相应的划分方式：

我们引用[这篇论文](https://arxiv.org/pdf/2312.10997)里的一张示意图：

&lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        &lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        

&lt;figure
  
&gt;
  &lt;picture&gt;
    &lt;!-- Auto scaling with imagemagick --&gt;
    &lt;!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    --&gt;
    
      &lt;source
        class="responsive-img-srcset"
        srcset="/assets/img/2025-10-01-ce101-5-rag/1759293255_8-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293255_8-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293255_8-1400.webp 1400w,"
        
          sizes="95vw"
        
        type="image/webp"
      &gt;
    
    &lt;img
      src="/assets/img/2025-10-01-ce101-5-rag/1759293255_8.png"
      
        class="img-fluid rounded z-depth-1"
      
      
        width="100%"
      
      
        height="auto"
      
      
      
      
      
        data-zoomable
      
      
        loading="eager"
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    &gt;
  &lt;/picture&gt;

  
&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
可以较为清楚的看出差别，分类是人为划分的，本质上就是针对基础的RAG在各个环节进行优化提升，目的都是为了提高最后输出的效果。

**进阶RAG（Advanced RAG）**就是加入了**前处理阶段（Pre-Retrieval）**来优化查询，比如查询重写或运用一些策略进行处理。并且加入了**后处理阶段（Post-Retrieval）**来优化检索后的文档块，比如重排、压缩或融合等手段，这样在最终给到大模型可以得到更好的结果提升。

**模块化RAG（Modular RAG）**则是将各种阶段或者功能单独成模块，每个模块是最小单元，可以自由的组合，形成一个类似workflow的流程，有点像是玩乐高积木，可以针对不同的业务场景自由组合。本质上里面的技术和方法没有变化，只不过是在工程化上进行了优化，方便不断复用和自由编排。

**图RAG（Graph RAG）**就是利用了图来辅助处理，万物皆可图，图的能力应用在RAG里，使得RAG得到了极大的提升，后面我们会在图RAG章节里会详细分析加入图能力，RAG得到的好处和提升。

**智能体RAG（Agentic RAG）**则是将RAG从简单的检索生成扩展成自主的Agent，可以基于一定的策略动态决策并进行多轮次检索，这个其实是对多跳RAG的一种提升，将AI Agent的思想融入RAG。

到这里我们再回过头来看看我们前面的那张架构图：

&lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        &lt;div class="row mt-3"&gt;
    &lt;div class="col-sm mt-0 mb-0"&gt;
        

&lt;figure
  
&gt;
  &lt;picture&gt;
    &lt;!-- Auto scaling with imagemagick --&gt;
    &lt;!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    --&gt;
    
      &lt;source
        class="responsive-img-srcset"
        srcset="/assets/img/2025-10-01-ce101-5-rag/1759293256_9-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293256_9-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293256_9-1400.webp 1400w,"
        
          sizes="95vw"
        
        type="image/webp"
      &gt;
    
    &lt;img
      src="/assets/img/2025-10-01-ce101-5-rag/1759293256_9.png"
      
        class="img-fluid rounded z-depth-1"
      
      
        width="100%"
      
      
        height="auto"
      
      
      
      
      
        data-zoomable
      
      
        loading="eager"
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    &gt;
  &lt;/picture&gt;

  
&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
这里面其实已经体现了很多的东西，我们可以把RAG分为：
1. 输入：可能有不同的输入方式，主流常见的是从Chat进来的问题
2. 前处理：检索前作一些前置处理动作，目的是增加召回效果
3. 检索：执行检索
4. 后处理：对检索的结果进行特定的处理，目的也是增加召回效果
5. 生成：给大模型输出最后的结果
6. 输出：将结果返回

这个其实就是一个进阶RAG的流程了，至于模块化RAG，其实是将里面的功能模块都单独抽出来形成独立的单元，这样可以重复自由组织编排，而图RAG和智能体RAG则会在里面多个环节参与。下面我们会针对一些关键的节点和方式展开。

## 6.2.1 查询重写

在传统的RAG里，通常就是将查询通过向量化的手段转成嵌入（embedding），做相似性搜索后给到大模型。这种情况下有明显可见的问题：**输入查询无法顺利匹配到文档块**。

在实际场景下，用户输入的问题有可能因为过于简化或者表述不当而无法通过相似度搜索匹配到合适的文档块，使得最终的效果不符合预期。面对这个问题，可以应用查询重写来进一步缓解并提升效果。

正如前面提到的，重写策略其实有挺多的，目前主流的有这么几种（更多还是一些类别的划分，实际上在不同的业务场景下还会有不同的策略浮现的，比如一些行业词汇重写、黑白词等等，这边就不过度展开）：

1. **规范化重写（Canonicalization）**：将随意、模糊、口语化表达转成标准清晰的问题
2. **同义改写（Paraphrasing）**：增强表达覆盖、抗embedding漏召
3. **泛化重写（Step-Back Query）**：提升复杂问题检索效果
4. **多查询生成（Multi-query Generation）**：多视角覆盖、提升召回率
5. **问题分解策略（Question Decomposition）**：将复杂查询拆分为多个子问题，分步检索和推理

### **规范化重写（Canonicalization）**

规范化重写其实就是针对查询问题让大语言模型帮忙进行重写，使得问题更加规范化，这其中有一些不同的手法。我们先来看一个基础的示例：

```plain text
周杰伦第一张专辑是什么？
</code></pre></div></div> <p>可以改写成</p> <p>```plain text 周杰伦第一张音乐专辑名称是什么？</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
类似这样的规范化重写，可以将一个较为随意的问题转变成更加正式的问题。以便在向量化检索的过程中，可以更好的召回预期的文档块用于最终的结果生成。

### **同义改写（Paraphrasing）**

同义改写的原理也是差不多的，对于不合适的表述，可以进行同义替换改写，使得输入的内容可以更容易匹配到合适的文档块。比如：

```plain text
# 历史聊天记录
User: 马斯克现在拥有哪些公司
AI: 截至2025年，马斯克拥有或主导的公司包括特斯拉、SpaceX、xAI（含X）、Neuralink 和 The Boring Company。
User: 他现在个人财富估值是多少？
</code></pre></div></div> <p>历史信息已经出现过相应的人物名，但是在最新的Query中却没有重复表述，此时是可以通过重写将用户最新的问题重写成：</p> <p>```plain text 马斯克现在个人财富估值是多少？</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
甚至是可以进一步结合前面规范化重写：

```plain text
截止2025年7月，马斯克（Elon Musk）的个人净资产估值是多少？

</code></pre></div></div> <p>这样等于是把时间具体化，并且名词也更加规范化表述了。</p> <h3 id="泛化重写step-back-query"><strong>泛化重写（Step-Back Query）</strong></h3> <p>泛化重写是把具体的问题抽象，将问题覆盖范围扩大了，这样可以扩大检索范围和获取更完整的上下文信息，比如：</p> <p>```plain text 马斯克的出生地是哪里？</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
可以改写成：

```plain text
 马斯克的个人背景和早年经历是什么？
</code></pre></div></div> <p>这种好处不是明显可见的，为什么这么说呢？因为问题被泛化之后，有可能会导致答案也进一步被泛化，当然最终的递送给大语言模型的Prompt是可以保留原始的问题的。泛化重写其实是应该结合多跳RAG这些技术来发挥更大的作用，这个在后续我们也会涉及到，简单说就是通过泛化先在一个方向上探索，再一步步细化定位到实际想要的结果中。</p> <h3 id="多查询生成multi-query-generation"><strong>多查询生成（Multi-query Generation）</strong></h3> <p>这个方式也是应对用户问题表述不清晰或含糊的情况，通过将单一问题生成多个问题的方式，对一个问题提供多个角度，这样可以提高覆盖度，达到更好的检索和结果生成效果。</p> <p>我们来看下例子：</p> <p>```plain text 周杰伦的第一张专辑是什么？</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
多查询重写：

```plain text
周杰伦最早发行的专辑是哪一张？
周杰伦第一张音乐专辑的名字是什么？
周杰伦早期的音乐作品有哪些？
周杰伦的音乐出道作品是哪一张专辑？
</code></pre></div></div> <p>这样就将一个问题扩展出基于不同角度的多个问题组合，这样可以以较为全面的角度去召回文档块了。</p> <h3 id="问题分解策略question-decomposition"><strong>问题分解策略（Question Decomposition）</strong></h3> <p>将一个复杂问题拆解成多个原子问题，使得可以基于多个问题去分别召回文档块，比如：</p> <p>```plain text 周杰伦从出道到现在有哪些重要的音乐成就？</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
可以拆解成：

```plain text
周杰伦是哪一年出道的？
周杰伦的第一张专辑是什么？
周杰伦获得过哪些音乐奖项？
周杰伦的代表作有哪些？
他对华语乐坛的影响体现在哪些方面？
</code></pre></div></div> <p>这样可以基于不同的问题去做处理了。这里其实还可以结合前面的一些重写策略进一步完善子问题。</p> <p>另外这种方式通常会结合一些MapReduce的思维去做时间，也就是基于不同的原子问题去做文档块的召回，并做不同的结果生成，最终再把所有的结果再进行汇总生成一个最终的结果。后续我们也会提到这块应用，尤其在Graph RAG里有很完备的应用示例可以学习。</p> <h2 id="622-检索结果重排">6.2.2 检索结果重排</h2> <p>重排是提升RAG检索效果里很重要的一步，也是目前实际应用中很广泛被采用的一种方式，主要有几种方式：</p> <ol> <li> <strong>基于打分函数的传统重排方法</strong>：BM25，TF-IDF余弦相似度</li> <li> <strong>语义匹配类重排方法</strong>：双塔结构（Bi-Encoder），交叉编码器（Cross-Encoder）</li> <li> <strong>生成式重排方法</strong>：通过LLM进行评分和排序</li> </ol> <p>实际使用需要根据业务需求和所有的资源来决定，这边我们来看个例子，LangChain官方有一个<a href="https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/" rel="external nofollow noopener" target="_blank">FlashRank reranker</a>的例子，采用的是<a href="https://github.com/PrithivirajDamodaran/FlashRank" rel="external nofollow noopener" target="_blank">FlashRank</a>，主要支持Pointwise（单文档打分），Pairwise（双文档比较，看谁相关度更好）和Listwise（列表排序，一次对所有文档排序）两种方式</p> <p>下面是一个基础的RAG流程，对文档切分后建立embedding，然后在对问题做向量化后在里面检索出相似度最高的20条文档片段</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="n">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">documents</span> <span class="o">=</span> <span class="nc">TextLoader</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">../../how_to/state_of_the_union.txt</span><span class="sh">"</span><span class="p">,</span>
<span class="p">).</span><span class="nf">load</span><span class="p">()</span>
<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="n">text</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">text-embedding-ada-002</span><span class="sh">"</span><span class="p">)</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">embedding</span><span class="p">).</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">:</span> <span class="mi">20</span><span class="p">})</span>

<span class="n">query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What did the president say about Ketanji Brown Jackson</span><span class="sh">"</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nf">pretty_print_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

</code></pre></div></div> <p>现在来应用一下FlashRank做重排，从前面读取<code class="language-plaintext highlighter-rouge">retriever</code>，构建<code class="language-plaintext highlighter-rouge">ContextualCompressionRetriever</code>，里面会使用<code class="language-plaintext highlighter-rouge">FlashrankRerank</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.retrievers</span> <span class="kn">import</span> <span class="n">ContextualCompressionRetriever</span>
<span class="kn">from</span> <span class="n">langchain_community.document_compressors</span> <span class="kn">import</span> <span class="n">FlashrankRerank</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">compressor</span> <span class="o">=</span> <span class="nc">FlashrankRerank</span><span class="p">()</span>
<span class="n">compression_retriever</span> <span class="o">=</span> <span class="nc">ContextualCompressionRetriever</span><span class="p">(</span>
    <span class="n">base_compressor</span><span class="o">=</span><span class="n">compressor</span><span class="p">,</span> <span class="n">base_retriever</span><span class="o">=</span><span class="n">retriever</span>
<span class="p">)</span>

<span class="n">compressed_docs</span> <span class="o">=</span> <span class="n">compression_retriever</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">What did the president say about Ketanji Jackson Brown</span><span class="sh">"</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">([</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">compressed_docs</span><span class="p">])</span>

</code></pre></div></div> <p>对比一下前后的效果：</p> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-10-01-ce101-5-rag/1759293257_10-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293257_10-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293257_10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-10-01-ce101-5-rag/1759293257_10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </div> <ul> <li>Document 1 -&gt; Document 1</li> <li>Document 4 -&gt; Document 2</li> <li>Document 6 -&gt; Document 3 经过重排后，获取到的Top 3文档不一样了</li> </ul> <h2 id="623-graph-rag">6.2.3 Graph RAG</h2> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-10-01-ce101-5-rag/1759293257_11-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293257_11-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293257_11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-10-01-ce101-5-rag/1759293257_11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </div> <p><a href="https://microsoft.github.io/graphrag/" rel="external nofollow noopener" target="_blank">Graph RAG</a>是微软在2024年推出的一种结构化、分层的检索增强生成（RAG）方法，相较于仅使用纯文本片段进行语义搜索的朴素方法，它更加系统和智能。GraphRAG 的处理流程包括：从原始文本中提取知识图谱、构建社区层级结构、为这些社区生成摘要，并在执行基于 RAG 的任务时充分利用这些结构化信息。下面我们会做一个比较详细的分析</p> <h3 id="索引阶段">索引阶段</h3> <p>看看架构图可以有个全局的认知</p> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <div class="row mt-3"> <div class="col-sm mt-0 mb-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-10-01-ce101-5-rag/1759293258_12-480.webp 480w,/assets/img/2025-10-01-ce101-5-rag/1759293258_12-800.webp 800w,/assets/img/2025-10-01-ce101-5-rag/1759293258_12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-10-01-ce101-5-rag/1759293258_12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </div> <p>我们来看看标准处理流程：</p> <ol> <li>文本处理 （Text Processing）</li> <li>文档处理（Document Processing）</li> <li>图提取（Graph Extraction）</li> <li>图增强（Graph Augmentation）</li> <li>声明提取（Claims Extraction）</li> <li>社区创建（Community Creation）</li> <li>文本单元最终化（(Final Text Units）</li> <li>社区报告生成（Community Reports）</li> <li>文本嵌入（Text Embeddings）</li> </ol> <h3 id="文本处理-text-processing">文本处理 （Text Processing）</h3> <p>主要接收多种数据输入，然后对输入的数据进行<strong>切分</strong>（支持按句子或者token进行切分），分块得到<strong>文本单元TextUnits</strong>。</p> <p>这步主要是为了<strong>方便后续的数据处理</strong>，因为后续的处理涉及多轮次的模型调用，以一个合理块大小的处理单元来处理，会更加方便且上下文不容超过，<strong>也适合并发调度处理</strong>。</p> <h3 id="文档处理document-processing">文档处理（Document Processing）</h3> <p>将文本处理阶段处理出来的TextUnits与原始文档建立引用关系，形成一个<strong>结构化的数据表</strong>，用于后续一些操作：</p> <ul> <li>跟踪每个文档包含哪些chunk</li> <li>后续社区摘要、图构建等流程中使用</li> <li>统一文档展示和可视化索引</li> </ul> <h3 id="图提取graph-extraction">图提取（Graph Extraction）</h3> <p>会包含几个阶段：</p> <ol> <li> <strong>实体（Entity）</strong>和<strong>关系（Relationship）</strong>提取</li> <li>图数据进行摘要简化（Graph Summrization）</li> </ol> <p>首先会让大语言模型提取文本里的<strong>实体（Entity）</strong>，以及不同实体间的<strong>关系（Relationship）</strong>，还会附带<strong>关系强弱的评分</strong>用于<strong>计算实体间的关系权重</strong>。</p> <p>这期间会在内存中做一定的合并和更新。比如实体和关系的描述，持续的更新会导致描述膨胀，这种情况下需要再进行一步图摘要，也就是让模型再次帮忙将实体和关系里的描述做总结为单一简介描述</p> <h3 id="图增强graph-augmentation">图增强（Graph Augmentation）</h3> <p>图增强里主要是图<strong>最终化</strong>，也就是将初步提取出来的图数据（实体节点和关系边），经过清洗、加工、标准化并准备好用于下游使用的过程。因为这是图构建的最后阶段：</p> <ul> <li>之前：只有基础的实体名称、描述、关系</li> <li>之后：实体具备了向量表示、空间坐标、网络属性等完整特征</li> </ul> <p>简单说就是： <strong>初步提取的基础数据 -&gt; 可用于可视化、推理、检索和分析的结构化图</strong></p> <p>在对实体最终化流程中，会有这么一些操作和步骤：</p> <ul> <li>根据配置决定是否创建向量（embedding）</li> <li>根据配置决定是否对图做UMAP或其他布局（layout）方法，生成2D/3D坐标用于可视化</li> <li>计算每个实体节点的度数（degree），用于后续分析或排序</li> <li>合并、移除重复、预填充缺失字段、生成唯一id等等</li> </ul> <blockquote> <p>UMAP（Uniform Manifold Approximation and Projection）中文名为统一流形近似与投影算法，是一种非线性降维算法，可以用于把高维数据（比如向量嵌入embedding）映射到二维或三维空间，用于方便可视化或聚类分析。简单说就是： UMAP 是一种可以把高维“云雾向量”压缩成漂亮二维坐标点的方法，保留结构、方便展示和聚类</p> </blockquote> <p>关于实体节点的<strong>度数（degree）</strong>，其实是每个节点连接的边的数量，比如：</p> <ul> <li>Leo –写–&gt; 书</li> <li>Leo –开发–&gt; 应用</li> </ul> <p>那么Leo这个节点就有两条边，它的degree就是2。那为什么要算degree呢？因为在图分析/图机器学习中，degree是一个很有用的特征值，比如：</p> <ul> <li>找到重要节点：高度数可能表示实体在图中很核心</li> <li>控制布局：在图布局中（比如UMAP或Force-directed），高 degree 节点更可能在中心。</li> <li>下游模型特征：在图神经网络中，degree 是常用的节点特征之一</li> <li>图过滤：有时我们只保留degree&gt;=2的节点，忽略孤立点（degree=0）。</li> </ul> <h3 id="声明提取claims-extraction">声明提取（Claims Extraction）</h3> <p>Graph RAG里面是叫做<strong>共变量（Covariates）提取任务</strong>，一个道理，就是从文本单元里提取声明（Claims）的过程，并将其转换为结构化数据，供后续图构建或社区摘要使用。</p> <p>操作主要是让<strong>模型针对文本单元里的内容进行声明提取</strong>，Prompt里会包括实体、想找的主张，需要分析的原始内容，最终模型会输出声明主体、涉及对象、声明类型、声明状态（对/错/存疑）、时间范围、描述说明、原始文本这些信息。</p> <h3 id="社区创建community-creation">社区创建（Community Creation）</h3> <p>这里会借助Leiden算法将节点进行<strong>社区化</strong>，简单说就是<strong>把相似、相关的阶段放到统一个社区</strong>。社区是指内部连接多，外部连接少的一组节点，类比班级，一个班级内部的同学联系较为紧密，而不同的班级之间的联系相对就少一点，这里班级就是一个社区的概念。另外同一个班级之下还可以分兴趣小组，这样就出现了分层级的社区，也就是某个社区有可能归属于某个父社区。Leiden算法整体就是在做这么一件事情，我们不展开算法的细节，有兴趣的可以自行了解。</p> <p>通过构建，最终是可以得到一个这种结构的数据</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">level</span><span class="p">,</span> <span class="n">cluster_id</span><span class="p">,</span> <span class="n">parent_cluster_id</span><span class="p">,</span> <span class="p">[</span><span class="n">node_ids</span><span class="p">])</span>
</code></pre></div></div> <p>示例数据</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">]),</span>  <span class="c1"># 一级社区，ID=1，父节点=-1（说明是顶层），含有节点A/B/C
</span>  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">]),</span>        <span class="c1"># 二级社区，ID=2，父节点是1，细分A/B
</span><span class="p">]</span>
</code></pre></div></div> <p>最终再通过一定的操作来<strong>整理聚合社区</strong>，只保留每个社区里实体和社区内实体间关系信息，社区之间的关系被忽略，这样最终就得到一份社区数据了，会存放到数据库里，类似</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">id</span><span class="p">,</span><span class="n">human_readable_id</span><span class="p">,</span><span class="n">community</span><span class="p">,</span><span class="n">parent</span><span class="p">,</span><span class="n">children</span><span class="p">,</span><span class="n">entity_ids</span><span class="p">,</span><span class="n">relationship_ids</span><span class="p">,</span><span class="n">text_unit_ids</span><span class="p">,</span><span class="n">level</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">period</span><span class="p">,</span><span class="n">size</span>
<span class="mf">1e2</span><span class="n">f3a00</span><span class="o">-</span><span class="n">aaaa</span><span class="o">-</span><span class="mi">1111</span><span class="o">-</span><span class="n">bbbb</span><span class="o">-</span><span class="mi">000000000001</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="sh">"</span><span class="s">[]</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">[</span><span class="sh">'</span><span class="s">e1</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">e2</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">e3</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">[</span><span class="sh">'</span><span class="s">r1</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">r2</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">[</span><span class="sh">'</span><span class="s">t1</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">t2</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">t3</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">Community</span> <span class="mi">0</span><span class="p">,</span><span class="mi">2025</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span><span class="mi">3</span>
<span class="mi">4</span><span class="n">a6b7c00</span><span class="o">-</span><span class="n">bbbb</span><span class="o">-</span><span class="mi">2222</span><span class="o">-</span><span class="n">cccc</span><span class="o">-</span><span class="mi">000000000002</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="sh">"</span><span class="s">[]</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">[</span><span class="sh">'</span><span class="s">e4</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">e5</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">[</span><span class="sh">'</span><span class="s">r3</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">[</span><span class="sh">'</span><span class="s">t4</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">t5</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">Community</span> <span class="mi">1</span><span class="p">,</span><span class="mi">2025</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span><span class="mi">2</span>
</code></pre></div></div> <h3 id="文本单元最终化final-text-units">文本单元最终化（(Final Text Units）</h3> <p>这一步主要是针对前面的几个步骤产生的<strong>中间数据做最终的聚合关联</strong>，也就是将文本单元（TextUnits）与实体（Entities）、关系（Relationships）和声明共变量（Covariates）。关联之后文本单元就拥有了实体id列表、关系列表、声明列表。</p> <p>大概数据如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="p">{</span>
      <span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">text_unit_001</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">short_id</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Apple Inc. is headquartered in Cupertino...</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">n_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">127</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">document_ids</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">doc_001</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">doc_002</span><span class="sh">"</span><span class="p">],</span>
      <span class="sh">"</span><span class="s">entity_ids</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">entity_apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">entity_cupertino</span><span class="sh">"</span><span class="p">],</span>      <span class="c1"># ⭐ 图数据关联
</span>      <span class="sh">"</span><span class="s">relationship_ids</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">rel_001</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rel_002</span><span class="sh">"</span><span class="p">],</span>              <span class="c1"># ⭐ 图数据关联
</span>      <span class="sh">"</span><span class="s">covariate_ids</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">claim_001</span><span class="sh">"</span><span class="p">]</span>                           <span class="c1"># ⭐ 声明数据关联
</span>  <span class="p">}</span>
</code></pre></div></div> <p>这步的目的是为每个文本单元添加结构化语义（实体、关系、属性），为后续图创建和问答系统打下基础。</p> <h3 id="社区报告生成community-reports">社区报告生成（Community Reports）</h3> <p>这步核心目的是基于实体（Entities）、关系（Relationships）、社区（Communities）和声明（Claims），构建每个社区的<strong>摘要性报告</strong>。</p> <p>核心的处理步骤有：</p> <ul> <li>社区展开：将社区结构展开</li> <li>数据准备：预处理实体、关系和声明数据</li> <li>上下文创建：为每个社区构建上下文</li> <li>摘要生成：生成社区报告</li> </ul> <p>首先就是将原本的社区记录（一条记录是一个社区，包含多个实体和关系）展开，然后合并到实体里，这样实体里就包含了所属社区、层级这些信息了。</p> <p>然后就是针对实体、关系和声明做相应的结构化数据准备，补充一些缺失的描述，为后续构建Prompt做准备。</p> <p>接下去是针对每个社区构建一份<strong>本地上下文（Local Context）</strong>。首先会遍历社区的所有层级（从高到低，这边可以理解一层都有不同的社区，上层的社区下会继续划分子社区，所以是一个嵌套关系的），对每个社区聚合实体、边、声明，然后将结构化的社区上下文变成模型可读的Prompt，再发送给模型进行摘要。</p> <p>摘要生成主要是读取前一步产生的社区上下文信息，调用大语言模型去生成文字摘要。期间会有一些车略，比如处理上下超限的情况，会尝试用子社区报告替换本地上下文，如果无法替换则进行修剪本地上下文以适应限制。</p> <p>样例数据：</p> <p>```plain text —–Reports—– community_id,full_content 1,”Community 1 consists of software development entities focused on healthcare applications…”</p> <p>—–Entities—– id,entity,description,degree 5,MICROSOFT,Microsoft is a technology company,15 12,AZURE CLOUD,Azure is Microsoft’s cloud computing platform,8 23,HEALTHCARE APP,A healthcare application developed by Microsoft,3</p> <p>—–Relationships—– id,source,target,description,degree 101,MICROSOFT,AZURE CLOUD,Microsoft owns and operates Azure Cloud platform,12 102,AZURE CLOUD,HEALTHCARE APP,Healthcare app is deployed on Azure Cloud,6</p> <p>—–Claims—– id,subject,type,status,description 201,MICROSOFT,CLAIM,CONFIRMED,Microsoft has strong presence in healthcare technology 202,HEALTHCARE APP,CLAIM,SUSPECTED,The app may have compliance issues</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
### 文本嵌入（Text Embeddings）

这步是最后的环节了，用于为前面产生的各种文本内容生成对应的**向量表示**，用于后续检索阶段的语义搜索和向量检索。主要包括：

- 完整文档内容
- 实体标题和描述
- 关系描述
- 文本单元
- 社区标题和摘要
- 社区完整报告内容

### 检索阶段

Graph RAG针对不同的使用场景，提供了4种查询方法：

1. **全局搜索（Global Search）**：面向社区报告级别的全局搜索，适合高层知识查找
2. **本地搜索（Local Search）**：走了图和文本搜索，同时融合实体、关系、文本等细粒度搜索
3. **动态推理搜索（DRIFT Search）**：和本地搜索类似，但是引入了embedding对齐
4. **基础搜索（Basic Search）**：走了文本级别的搜索，是最轻量的文本向量语义检索

### **全局搜索（Global Search）**

主要**基于社区（Community）和其报告（Reports）进行粗粒度搜索**。走的是Map Reduce的方式，也就是将社区报告拆成多个文本块（chunks），每个文本块分别发送给大语言模型做分析，会生成类似下面格式的内容

```plain text
points,
        description
    ]
}}
</code></pre></div></div> <p>这里包括的是对应社区报告的摘要，精炼的内容描述和对应的重要性得分，评分会决定该观点是否值得被纳入最终的Reduce阶段。Reduce阶段只会过滤出score大于0的结果，并且对结果进行排序，使得较为重要的观点排在前面，最终会展现出类似这样的形式：</p> <p>```plain text —-Analyst 1—- Importance Score: 90 某个摘要句子…</p> <p>—-Analyst 2—- Importance Score: 88 另一个摘要句子…</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
表现出不同的“分析员”（Analyst）的分析情况，然后把这份汇总的结果再次发送到大语言模型，将多个“分析员”的观点汇总成一个连贯、有逻辑且可读性较强的最终答案。输入的prompt片段类似：

```plain text
---Target response length and format---

Multi-paragraph explanation with markdown headings

---Analyst Reports---

----Analyst 1----
Importance Score: 95
Company A violated environmental regulations in 2021 and was fined [Data: Reports (3, 6, 7)].

----Analyst 2----
Importance Score: 82
Whistleblowers from 2020 also claimed unsafe disposal methods by Company A [Data: Reports (12, 15, 19, 22, 26, +more)].
</code></pre></div></div> <p>最终输出的类似：</p> <p>```plain text</p> <h2 id="environmental-violations-of-company-a">Environmental Violations of Company A</h2> <p>Company A was found guilty of violating environmental regulations in 2021, resulting in multiple fines [Data: Reports (3, 6, 7)].</p> <p>In addition, whistleblower reports from 2020 suggested unsafe disposal practices, further highlighting the company’s failure in compliance [Data: Reports (12, 15, 19, 22, 26, +more)].</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
### **本地搜索（Local Search）**

本地搜索会利用**向量搜索**去检索出**合适的实体（Entities）**，然后给予这个实体去构建对应的上下文，其中涉及到了以下的数据：

- 实体
- 关系
- 文本单元
- 社区摘要
- 声明

其中实体是通过向量化搜索得到的，社区则是通过排序后选出topK个社区摘要，其他的则是通过对应实体去检索。最终会将上面的这些数据构建成单个上下文（不像全局搜索用chunk的形式）。然后将这个上下文结合预设的Prompt一起发送到大语言模型生成结果。

示例输入片段：

```plain text
---Role---

You are a helpful assistant responding to questions about data in the tables provided.

...

---Target response length and format---

multi-paragraph summary

---Data tables---

Entities Table:
1. John Smith - CEO
2. ...
</code></pre></div></div> <p>输出示例：</p> <p>```plain text</p> <h2 id="key-individuals">Key Individuals</h2> <p>John Smith is listed as CEO of Company A [Data: Entities (1)].</p> <p>…</p> <h2 id="summary">Summary</h2> <p>These findings suggest …</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
### **动态推理搜索（DRIFT Search）**

动态推理搜索（DRIFT Search，Dynamic Reasoning and Inference with Flexible Traversal）是最复杂也最智能的一种检索方式，它结合了推理驱动的层次搜索、查询拆分（Primer）、多步骤搜索和最终答案的合并（Reduce）。

首先DRIFT会随机从社区报告里取一个**全量文本**出来，然后将输入的内容与随机取出的社区报告（作为模板）给到大语言模型去做相应的**虚拟答案（Hypothetical Answer）**生成，相应的Prompt是这样的：

```plain text
Create a hypothetical answer to the following query: {query}

Format it to follow the structure of the template below:

{template}

Ensure that the hypothetical answer does not reference new named entities that are not present in the original query.
</code></pre></div></div> <p>然后将虚拟的答案转成向量，通过计算余弦相似度（Sosine Similarity），可以得到虚拟答案和所有文档的相似度，取出topK社区报告。</p> <p>然后基于Primer做将topK社区报告进行分片，并发调用LLM对每一份报告进行子问题生成（Query Decomposition）。我们来看看其Prompt模板：</p> <p>```plain text You are a helpful agent designed to reason over a knowledge graph in response to a user query. This is a unique knowledge graph where edges are freeform text rather than verb operators. You will begin your reasoning looking at a summary of the content of the most relevant communites and will provide:</p> <ol> <li> <p>score: How well the intermediate answer addresses the query. A score of 0 indicates a poor, unfocused answer, while a score of 100 indicates a highly focused, relevant answer that addresses the query in its entirety.</p> </li> <li> <p>intermediate_answer: This answer should match the level of detail and length found in the community summaries. The intermediate answer should be exactly 2000 characters long. This must be formatted in markdown and must begin with a header that explains how the following text is related to the query.</p> </li> <li> <p>follow_up_queries: A list of follow-up queries that could be asked to further explore the topic. These should be formatted as a list of strings. Generate at least five good follow-up queries.</p> </li> </ol> <p>Use this information to help you decide whether or not you need more information about the entities mentioned in the report. You may also use your general knowledge to think of entities which may help enrich your answer.</p> <p>You will also provide a full answer from the content you have available. Use the data provided to generate follow-up queries to help refine your search. Do not ask compound questions, for example: “What is the market cap of Apple and Microsoft?”. Use your knowledge of the entity distribution to focus on entity types that will be useful for searching a broad area of the knowledge graph.</p> <p>For the query:</p> <p>{query}</p> <p>The top-ranked community summaries:</p> <p>{community_reports}</p> <p>Provide the intermediate answer, and all scores in JSON format following:</p> <p>intermediate_answer</p> <p>Begin:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
这里的Prompt要求LLM以类人类推理者而不是抽象逻辑机器来推理，其作用是结合用户query与社区总结（community reports），引导LLM推理出一个中间答案（intermediate answer）和一组后续子查询（follow-up queries）

输出示例：

```plain text
{
  "intermediate_answer": "## Challenges Faced by EV Companies in 2024\n\nElectric vehicle companies encountered several critical challenges in...",
  "score": 91,
  "follow_up_queries": [
    "How are EV companies addressing battery material shortages?",
    "What trade policies are affecting Chinese EV exports?",
    "What steps is Tesla taking to resolve labor disputes in Berlin?",
    "How are legacy automakers improving their software capabilities?",
    "What impact do rising raw material costs have on EV pricing in 2024?"
  ]
}
</code></pre></div></div> <p>最终这个环节得到的是以虚拟答案检索出来的topK社区报告为语境种子，去生成对应的中间答案和子查询列表以及对应的评分。最后就是将所有的中间答案拼接起来，评分取平均数，子查询问题合并。</p> <p>接下去进入到循环执行动作（Action）的步骤了，会持续从当前状态中挑出尚未处理的动作（只保留top-k最重要的动作），每个动作进行搜索，这里的检索走的是本地搜索（Local Search），也就是针对query走图和文本搜索。这边还会控制最大深度，避免深度爆炸。</p> <p>最后将所有的结果进行聚合（Reduce），会将前面所有Action最终的回答拼接让模型帮忙汇总出最终答案</p> <h3 id="基础搜索basic-search"><strong>基础搜索（Basic Search）</strong></h3> <p>基础搜索的话只会将问题基于文本单元做<strong>向量检索</strong>，得到topK结果，然后到大语言模型进行生成。相对简单的一个检索。</p> <p>示例输入：</p> <p>```plain text source_id|text 12|John Smith is the CEO of QuantumTech and has faced several allegations of insider trading. 34|QuantumTech has been under investigation by the SEC since 2022. 46|Multiple anonymous reports accuse John Smith of misusing company resources. 51|John Smith was previously CEO at FutureCorp, where a similar scandal occurred. 55|Internal emails obtained by regulators suggest conflicts of interest involving John Smith.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
示例输出：

```plain text
---Target response length and format---

multiple paragraphs

---Data tables---

source_id|text
12|John Smith is the CEO of QuantumTech and has faced several allegations of insider trading.
34|QuantumTech has been under investigation by the SEC since 2022.
46|Multiple anonymous reports accuse John Smith of misusing company resources.
51|John Smith was previously CEO at FutureCorp, where a similar scandal occurred.
55|Internal emails obtained by regulators suggest conflicts of interest involving John Smith.
</code></pre></div></div> <h3 id="总结">总结</h3> <p>我们花了很长的篇幅来深入GraphRAG，是因为我觉得里面应用了很多相关技术实现，从最基础的向量化检索，到采用了图做结合，甚至里面也融合了多跳RAG或者说多跳推理的技术，还利用了HyDE（Hypothetical Response）的思想。因此非常值得深入了解和学习。</p> <p>总体而言Graph RAG通过将非结构化文本转化为图结构表示，突破了传统 RAG 仅依赖向量检索的局限性。它采用分阶段处理流程，从文本中提取实体与关系，构建社区结构与摘要信息，并融合图结构与向量嵌入，实现多种检索模式的协同支持。</p> <p>在复杂上下文与多样应用场景中，GraphRAG 提供了一个强有力的实践范式。尽管本质上仍受限于语言模型的上下文窗口，但它通过算法、工程与架构手段最大化信息利用效率，将原本偏单跳的 RAG 推进到更具多跳推理能力的方向。其核心目标始终是：<strong>获取最相关、最有用的上下文以支持更好的生成结果。</strong></p> <p>RAG这部分内容非常多，目前也只是走马观花式的覆盖了一部分内容，包括AgenticRAG在内的一些方式还没有展开篇幅去讲，但是我觉得整个篇幅的内容已经足够支撑每一位读者去开启RAG探索之路了。除了技术探索和学术研究以外，在Applied AI中，我们会更加关注实际的业务和需求，始终以此作为导向，利用技术去创造更多的商业价值，才是有意义的事情，因此技术不是目的而是手段，当我们遇到一个无法解决的问题时，或许应该再去看看业界有什么新的方法，如果刚好没有，就是创造这个新的方法的时候。</p> <p>那么我们就继续往下走，来看看工具之于上下文工程的意义和用法</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/yo-you-checked-in-30-times-bitch/">同学你好，你已经签到30次了</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leotalk-ai-weekly-1-qwen-is-on-fire/">LeoTalk AI周知 2: Qwen is on fire!</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/why-openai-built-the-responses-api/">为什么OpenAI要推出Responses API</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leotalk-hacker-news-daily-september-22-2025/">LeoTalk · Hacker News Daily · 2025.09.22</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leotalk-ai-weekly-1-an-experiment-in-information-e/">LeoTalk AI周知 1: 新信息排泄物实验</a> </li> <div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"iFurySt/ifuryst.github.io","data-repo-id":"R_kgDOMXF2RA","data-category":"General","data-category-id":"DIC_kwDOMXF2RM4ChdWx","data-mapping":"pathname","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Leo Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com/@ifuryst" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-6Y91Y4PCLJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-6Y91Y4PCLJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-\u5173\u4e8e",title:"\u5173\u4e8e",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-\u6587\u7ae0",title:"\u6587\u7ae0",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-\u751f\u6d3b",title:"\u751f\u6d3b",description:"life outside work",section:"Navigation",handler:()=>{window.location.href="/life/"}},{id:"post-\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c5\u7ae0-\u68c0\u7d22\u589e\u5f3a\u751f\u6210",title:"\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c5\u7ae0\uff1a\u68c0\u7d22\u589e\u5f3a\u751f\u6210",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/ce101-5-rag/"}},{id:"post-\u540c\u5b66\u4f60\u597d-\u4f60\u5df2\u7ecf\u7b7e\u523030\u6b21\u4e86",title:"\u540c\u5b66\u4f60\u597d\uff0c\u4f60\u5df2\u7ecf\u7b7e\u523030\u6b21\u4e86",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yo-you-checked-in-30-times-bitch/"}},{id:"post-leotalk-ai\u5468\u77e5-2-qwen-is-on-fire",title:"LeoTalk AI\u5468\u77e5 2: Qwen is on fire!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/leotalk-ai-weekly-1-qwen-is-on-fire/"}},{id:"post-\u4e3a\u4ec0\u4e48openai\u8981\u63a8\u51faresponses-api",title:"\u4e3a\u4ec0\u4e48OpenAI\u8981\u63a8\u51faResponses API",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/why-openai-built-the-responses-api/"}},{id:"post-leotalk-hacker-news-daily-2025-09-22",title:"LeoTalk \xb7 Hacker News Daily \xb7 2025.09.22",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/leotalk-hacker-news-daily-september-22-2025/"}},{id:"post-leotalk-ai\u5468\u77e5-1-\u65b0\u4fe1\u606f\u6392\u6cc4\u7269\u5b9e\u9a8c",title:"LeoTalk AI\u5468\u77e5 1: \u65b0\u4fe1\u606f\u6392\u6cc4\u7269\u5b9e\u9a8c",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/leotalk-ai-weekly-1-an-experiment-in-information-e/"}},{id:"post-\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c4\u7ae0-\u8bb0\u5fc6\u7cfb\u7edf\u4e0e\u6301\u4e45\u5316",title:"\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c4\u7ae0\uff1a\u8bb0\u5fc6\u7cfb\u7edf\u4e0e\u6301\u4e45\u5316",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/memory-and-persistence/"}},{id:"post-leotalk-hacker-news-daily-2025-09-15",title:"LeoTalk \xb7 Hacker News Daily \xb7 2025.09.15",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/leotalk-hacker-news-daily-september-15-2025/"}},{id:"post-leotalk-hacker-news-daily-2025-09-11",title:"LeoTalk \xb7 Hacker News Daily \xb7 2025.09.11",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/leotalk-hacker-news-daily-september-11-2025/"}},{id:"post-\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c3\u7ae0-\u63d0\u793a\u8bcd\u6280\u672f",title:"\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c3\u7ae0\uff1a\u63d0\u793a\u8bcd\u6280\u672f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/prompt-engineering-techniques/"}},{id:"post-leotalk-hacker-news-daily-2025-09-09",title:"LeoTalk \xb7 Hacker News Daily \xb7 2025.09.09",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/leotalk-hacker-news-daily-september-9-2025/"}},{id:"post-\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c2\u7ae0-\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u672f\u6808",title:"\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c2\u7ae0\uff1a\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u672f\u6808",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/context-engineering-stack/"}},{id:"post-gap\u4e86\u4e00\u4e2a\u5468\u672b",title:"Gap\u4e86\u4e00\u4e2a\u5468\u672b",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/gap-a-weekend-start-anew/"}},{id:"post-\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c1\u7ae0-\u4ece\u63d0\u793a\u8bcd\u5230\u4e0a\u4e0b\u6587",title:"\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u7b2c1\u7ae0\uff1a\u4ece\u63d0\u793a\u8bcd\u5230\u4e0a\u4e0b\u6587",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/from-prompt-engineering-to-context-engineering/"}},{id:"post-\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u5e8f\u7ae0",title:"\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5\u6307\u5357-\u5e8f\u7ae0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/ce-101-preface/"}},{id:"post-\u8d70\u51fa\u6d6a\u6d6a\u5c71-\u5c0f\u5996\u602a\u4e5f\u80fd\u5377\u8fdb\u5927\u5382\u4e86",title:"\u8d70\u51fa\u6d6a\u6d6a\u5c71\uff0c\u5c0f\u5996\u602a\u4e5f\u80fd\u5377\u8fdb\u5927\u5382\u4e86",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/a-nobody-s-way-into-big-tech/"}},{id:"post-gpt-5\u53d1\u5e03\u4f1a\u540e\u7684\u89c2\u5bdf\u4e0e\u8d8b\u52bf\u5206\u6790",title:"GPT-5\u53d1\u5e03\u4f1a\u540e\u7684\u89c2\u5bdf\u4e0e\u8d8b\u52bf\u5206\u6790",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/observations-and-trend-analysis-after-the-gpt-5-la/"}},{id:"post-\u6211\u7684\u5341\u516d\u5e74\u6280\u672f\u9010\u68a6",title:"\u6211\u7684\u5341\u516d\u5e74\u6280\u672f\u9010\u68a6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/my-16-year-journey-in-pursuit-of-tech/"}},{id:"post-\u6211\u7684\u5341\u516d\u5e74-2009-2014",title:"\u6211\u7684\u5341\u516d\u5e74\u2014\u20142009-2014",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sixteen-years-of-me-chapter-one-2009-2014/"}},{id:"post-ai\u8f85\u52a9\u7f16\u7a0b\u65f6\u4ee3-\u9762\u8bd5\u6d41\u7a0b\u5c06\u5982\u4f55\u6539\u53d8",title:"AI\u8f85\u52a9\u7f16\u7a0b\u65f6\u4ee3\uff0c\u9762\u8bd5\u6d41\u7a0b\u5c06\u5982\u4f55\u6539\u53d8\uff1f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/ai-is-changing-tech-interviews/"}},{id:"post-\u88abai\u97f3\u4e50\u5e7f\u544a\u6d17\u8111\u4e86",title:"\u88abAI\u97f3\u4e50\u5e7f\u544a\u6d17\u8111\u4e86",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/vibing-to-ai-generated-ad-music-on-youtube/"}},{id:"post-\u6211\u662f\u5982\u4f55\u57281\u5e74\u7684\u65f6\u95f4\u91cc\u6210\u4e3aai\u4e13\u5bb6\u7684",title:"\u6211\u662f\u5982\u4f55\u57281\u5e74\u7684\u65f6\u95f4\u91cc\u6210\u4e3aAI\u4e13\u5bb6\u7684",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/how-i-became-an-ai-expert-in-just-one-year/"}},{id:"post-chatgpt\u5b66\u4e60\u6a21\u5f0f",title:"ChatGPT\u5b66\u4e60\u6a21\u5f0f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/chatgpt-study-mode/"}},{id:"post-\u4ecemanus\u6784\u5efaai-agent\u770b\u4e0a\u4e0b\u6587\u5de5\u7a0b",title:"\u4eceManus\u6784\u5efaAI Agent\u770b\u4e0a\u4e0b\u6587\u5de5\u7a0b",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/context-engineering-at-manus/"}},{id:"post-ai\u65f6\u4ee3\u7684\u6559\u80b2\u53cd\u601d",title:"AI\u65f6\u4ee3\u7684\u6559\u80b2\u53cd\u601d",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/aiducation-ai-education/"}},{id:"post-agent-vs-human-pr\u6570\u636e\u91cc\u7684\u534f\u4f5c\u8d8b\u52bf",title:"Agent vs Human: PR\u6570\u636e\u91cc\u7684\u534f\u4f5c\u8d8b\u52bf",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/agent-vs-human-collaboration-trends-in-pr-data/"}},{id:"post-llm-\u662f\u5426\u5f00\u59cb\u6709\u610f\u8bc6",title:"LLM \u662f\u5426\u5f00\u59cb\u6709\u610f\u8bc6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/are-llms-starting-to-become-a-sentient/"}},{id:"post-cursor\u5b9a\u4ef7\u98ce\u6ce2\u7684\u80cc\u540e",title:"Cursor\u5b9a\u4ef7\u98ce\u6ce2\u7684\u80cc\u540e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/behind-the-cursor-pricing-controversy/"}},{id:"post-\u4e00\u6587\u770b\u61c2\u4e0a\u4e0b\u6587\u5de5\u7a0b-context-engineering",title:"\u4e00\u6587\u770b\u61c2\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08Context Engineering\uff09",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/context-engineering-101-what-it-is-and-why-it-matt/"}},{id:"post-\u8fd9\u4e2a\u8bcd\u6b63\u5728ai\u521b\u4e1a\u5708\u7206\u53d1\u529b\u91cf-momentum",title:"\u8fd9\u4e2a\u8bcd\u6b63\u5728AI\u521b\u4e1a\u5708\u7206\u53d1\u529b\u91cf\uff1aMomentum",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/momentum-for-startups/"}},{id:"post-claude-code\u5b9e\u6d4b\u62a5\u544a-\u5f53\u6211\u4e0d\u518ddebug",title:"Claude Code\u5b9e\u6d4b\u62a5\u544a: \u5f53\u6211\u4e0d\u518dDebug",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-experience/"}},{id:"post-\u5386\u53f2\u521a\u88ab\u521b\u9020\u51fa\u6765-\u800c\u4e16\u754c\u4e0a\u53ea\u6709\u5c11\u6570\u4eba\u77e5\u9053",title:"\u5386\u53f2\u521a\u88ab\u521b\u9020\u51fa\u6765\uff0c\u800c\u4e16\u754c\u4e0a\u53ea\u6709\u5c11\u6570\u4eba\u77e5\u9053",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/a-new-history-begins-known-only-to-a-few-copy/"}},{id:"post-33\u53f7\u8fdc\u5f81\u961f",title:"33\u53f7\u8fdc\u5f81\u961f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/expedition-33/"}},{id:"post-\u4ece\u4e92\u8054\u7f51\u7684\u5386\u53f2\u601d\u8003ai\u65f6\u4ee3",title:"\u4ece\u4e92\u8054\u7f51\u7684\u5386\u53f2\u601d\u8003AI\u65f6\u4ee3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/echoes-of-the-internet-in-the-ai-era/"}},{id:"post-google-io\u5f00\u53d1\u8005\u5c0f\u4f1a",title:"Google IO\u5f00\u53d1\u8005\u5c0f\u4f1a",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/google-io/"}},{id:"post-duolingo-w-ai",title:"Duolingo w/ AI",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/duolingo-with-ai/"}},{id:"post-\u5173\u4e8e\u8f6c\u5411\u706f\u7684\u601d\u8003",title:"\u5173\u4e8e\u8f6c\u5411\u706f\u7684\u601d\u8003",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/signals-we-miss/"}},{id:"post-mcp\u9274\u6743",title:"MCP\u9274\u6743",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/mcp-authorization/"}},{id:"post-mcp\u4e4b\u4e8ehttp",title:"MCP\u4e4b\u4e8eHTTP",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/mcp-vs-http/"}},{id:"post-iter-x-79-100days",title:"\u3010Iter-X\u3011 79/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-79-100/"}},{id:"post-iter-x-78-100days",title:"\u3010Iter-X\u3011 78/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-78-100/"}},{id:"post-iter-x-77-100days",title:"\u3010Iter-X\u3011 77/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-77-100/"}},{id:"post-iter-x-76-100days",title:"\u3010Iter-X\u3011 76/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-76-100/"}},{id:"post-iter-x-75-100days",title:"\u3010Iter-X\u3011 75/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-75-100/"}},{id:"post-iter-x-74-100days",title:"\u3010Iter-X\u3011 74/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-74-100/"}},{id:"post-iter-x-73-100days",title:"\u3010Iter-X\u3011 73/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-73-100/"}},{id:"post-iter-x-72-100days",title:"\u3010Iter-X\u3011 72/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-72-100/"}},{id:"post-iter-x-71-100days",title:"\u3010Iter-X\u3011 71/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-71-100/"}},{id:"post-iter-x-70-100days",title:"\u3010Iter-X\u3011 70/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-70-100/"}},{id:"post-trends1-\u5feb\u901f\u5206\u6790\u4ee3\u7801\u4ed3\u5e93",title:"[\ud83c\udf0aTrends1] \u5feb\u901f\u5206\u6790\u4ee3\u7801\u4ed3\u5e93",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/trends1-deeepwiki-quick-insights-on-repos/"}},{id:"post-iter-x-69-100days",title:"\u3010Iter-X\u3011 69/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-69-100/"}},{id:"post-iter-x-68-100days",title:"\u3010Iter-X\u3011 68/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-68-100/"}},{id:"post-iter-x-67-100days",title:"\u3010Iter-X\u3011 67/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-67-100/"}},{id:"post-iter-x-66-100days",title:"\u3010Iter-X\u3011 66/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-66-100/"}},{id:"post-iter-x-65-100days",title:"\u3010Iter-X\u3011 65/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-65-100/"}},{id:"post-iter-x-64-100days",title:"\u3010Iter-X\u3011 64/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-64-100/"}},{id:"post-iter-x-63-100days",title:"\u3010Iter-X\u3011 63/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-63-100/"}},{id:"post-iter-x-62-100days",title:"\u3010Iter-X\u3011 62/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-62-100/"}},{id:"post-iter-x-61-100days",title:"\u3010Iter-X\u3011 61/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-61-100/"}},{id:"post-iter-x-60-100days",title:"\u3010Iter-X\u3011 60/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-60-100/"}},{id:"post-iter-x-59-100days",title:"\u3010Iter-X\u3011 59/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-59-100/"}},{id:"post-iter-x-58-100days",title:"\u3010Iter-X\u3011 58/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-58-100/"}},{id:"post-iter-x-57-100days",title:"\u3010Iter-X\u3011 57/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-57-100/"}},{id:"post-iter-x-56-100days",title:"\u3010Iter-X\u3011 56/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-56-100/"}},{id:"post-iter-x-55-100days",title:"\u3010Iter-X\u3011 55/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-55-100/"}},{id:"post-iter-x-54-100days",title:"\u3010Iter-X\u3011 54/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-54-100/"}},{id:"post-iter-x-53-100days",title:"\u3010Iter-X\u3011 53/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-53-100/"}},{id:"post-iter-x-52-100days",title:"\u3010Iter-X\u3011 52/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-52-100/"}},{id:"post-iter-x-51-100days",title:"\u3010Iter-X\u3011 51/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-51-100/"}},{id:"post-iter-x-50-100days",title:"\u3010Iter-X\u3011 50/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-50-100/"}},{id:"post-iter-x-49-100days",title:"\u3010Iter-X\u3011 49/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-49-100/"}},{id:"post-iter-x-48-100days",title:"\u3010Iter-X\u3011 48/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-48-100/"}},{id:"post-iter-x-47-100days",title:"\u3010Iter-X\u3011 47/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-47-100/"}},{id:"post-iter-x-46-100days",title:"\u3010Iter-X\u3011 46/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-46-100/"}},{id:"post-iter-x-45-100days",title:"\u3010Iter-X\u3011 45/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-45-100/"}},{id:"post-mcp\u81ea\u5b9a\u4e49\u4f20\u8f93\u534f\u8bae",title:"MCP\u81ea\u5b9a\u4e49\u4f20\u8f93\u534f\u8bae",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/mcp-custom-transport-protocol/"}},{id:"post-iter-x-44-100days",title:"\u3010Iter-X\u3011 44/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-44-100/"}},{id:"post-iter-x-43-100days",title:"\u3010Iter-X\u3011 43/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-43-100/"}},{id:"post-iter-x-42-100days",title:"\u3010Iter-X\u3011 42/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-42-100/"}},{id:"post-iter-x-41-100days",title:"\u3010Iter-X\u3011 41/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-41-100/"}},{id:"post-iter-x-40-100days",title:"\u3010Iter-X\u3011 40/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-40-100/"}},{id:"post-iter-x-39-100days",title:"\u3010Iter-X\u3011 39/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-39-100/"}},{id:"post-iter-x-38-100days",title:"\u3010Iter-X\u3011 38/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-38-100/"}},{id:"post-iter-x-37-100days",title:"\u3010Iter-X\u3011 37/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-37-100/"}},{id:"post-iter-x-36-100days",title:"\u3010Iter-X\u3011 36/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-36-100/"}},{id:"post-iter-x-35-100days",title:"\u3010Iter-X\u3011 35/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-35-100/"}},{id:"post-iter-x-34-100days",title:"\u3010Iter-X\u3011 34/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-34-100/"}},{id:"post-iter-x-33-100days",title:"\u3010Iter-X\u3011 33/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-33-100/"}},{id:"post-iter-x-32-100days",title:"\u3010Iter-X\u3011 32/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-32-100/"}},{id:"post-iter-x-31-100days",title:"\u3010Iter-X\u3011 31/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-31-100/"}},{id:"post-iter-x-30-100days",title:"\u3010Iter-X\u3011 30/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-30-100/"}},{id:"post-iter-x-29-100days",title:"\u3010Iter-X\u3011 29/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-29-100/"}},{id:"post-iter-x-28-100days",title:"\u3010Iter-X\u3011 28/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-28-100/"}},{id:"post-iter-x-27-100days",title:"\u3010Iter-X\u3011 27/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-27-100/"}},{id:"post-iter-x-26-100days",title:"\u3010Iter-X\u3011 26/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-26-100/"}},{id:"post-iter-x-25-100days",title:"\u3010Iter-X\u3011 25/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-25-100/"}},{id:"post-iter-x-24-100days",title:"\u3010Iter-X\u3011 24/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-24-100/"}},{id:"post-iter-x-23-100days",title:"\u3010Iter-X\u3011 23/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-23-100/"}},{id:"post-iter-x-22-100days",title:"\u3010Iter-X\u3011 22/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-22-100/"}},{id:"post-iter-x-21-100days",title:"\u3010Iter-X\u3011 21/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-21-100/"}},{id:"post-iter-x-20-100days",title:"\u3010Iter-X\u3011 20/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-20-100/"}},{id:"post-iter-x-19-100days",title:"\u3010Iter-X\u3011 19/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-19-100/"}},{id:"post-iter-x-18-100days",title:"\u3010Iter-X\u3011 18/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-18-100/"}},{id:"post-iter-x-17-100days",title:"\u3010Iter-X\u3011 17/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-17-100/"}},{id:"post-iter-x-16-100days",title:"\u3010Iter-X\u3011 16/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-16-100/"}},{id:"post-iter-x-15-100days",title:"\u3010Iter-X\u3011 15/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-15-100/"}},{id:"post-\u4e2a\u4eba\u4e0e\u5927\u73af\u5883",title:"\u4e2a\u4eba\u4e0e\u5927\u73af\u5883",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/person-n-the-broader-ctx/"}},{id:"post-iter-x-14-100days",title:"\u3010Iter-X\u3011 14/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-14-100/"}},{id:"post-iter-x-13-100days",title:"\u3010Iter-X\u3011 13/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-13-100/"}},{id:"post-iter-x-12-100days",title:"\u3010Iter-X\u3011 12/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-12-100/"}},{id:"post-iter-x-11-100days",title:"\u3010Iter-X\u3011 11/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-11-100/"}},{id:"post-iter-x-10-100days",title:"\u3010Iter-X\u3011 10/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-10-100/"}},{id:"post-iter-x-9-100days",title:"\u3010Iter-X\u3011 9/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-9-100/"}},{id:"post-iter-x-8-100days",title:"\u3010Iter-X\u3011 8/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-8-100/"}},{id:"post-iter-x-7-100days",title:"\u3010Iter-X\u3011 7/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-7-100/"}},{id:"post-iter-x-6-100days",title:"\u3010Iter-X\u3011 6/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-6-100/"}},{id:"post-iter-x-5-100days",title:"\u3010Iter-X\u3011 5/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-5-100/"}},{id:"post-iter-x-4-100days",title:"\u3010Iter-X\u3011 4/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-4-100/"}},{id:"post-iter-x-3-100days",title:"\u3010Iter-X\u3011 3/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-3-100/"}},{id:"post-iter-x-2-100days",title:"\u3010Iter-X\u3011 2/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-2-100/"}},{id:"post-iter-x-1-100days",title:"\u3010Iter-X\u3011 1/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-1-100/"}},{id:"post-\u5206\u5e03\u5f0f\u9650\u6d41\u7b97\u6cd5",title:"\u5206\u5e03\u5f0f\u9650\u6d41\u7b97\u6cd5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/distributed-rate-limiter/"}},{id:"post-iter-x-0-100days",title:"\u3010Iter-X\u3011 0/100days",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/iter-x-0-100/"}},{id:"post-\u4ece\u8bed\u4e49\u5316\u6392\u5e8f\u770b\u672a\u6765\u7b97\u6cd5\u548c\u5de5\u7a0b\u7684\u53d8\u5316",title:"\u4ece\u8bed\u4e49\u5316\u6392\u5e8f\u770b\u672a\u6765\u7b97\u6cd5\u548c\u5de5\u7a0b\u7684\u53d8\u5316",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/llm-in-sort/"}},{id:"post-ai\u5728\u5e7f\u544a\u7684\u5e94\u7528",title:"AI\u5728\u5e7f\u544a\u7684\u5e94\u7528",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/ai-in-ads/"}},{id:"post-congrats-on-iclr-2025",title:"Congrats on ICLR 2025!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/congrats-on-iclr-2025/"}},{id:"post-back-online-let-s-rock-2025",title:"Back Online! Let\u2019s Rock 2025",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/lets-rock-2025/"}},{id:"post-my-2024-recap",title:"My 2024 Recap \ud83d\udcab",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/my-2024-recap/"}},{id:"post-default-ingress-class",title:"Default Ingress Class",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/default-ingress-class/"}},{id:"post-15s-1s\u6162\u67e5\u8be2\u4f18\u5316\u5c0f\u8bb0",title:"15s\u21921s\u6162\u67e5\u8be2\u4f18\u5316\u5c0f\u8bb0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/optimizing-slow-queries-from-15s-to-1s/"}},{id:"post-git\u591a\u9879\u76ee\u914d\u7f6e\u7ba1\u7406",title:"Git\u591a\u9879\u76ee\u914d\u7f6e\u7ba1\u7406",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/unified-git-management-for-multiple-git-repositories/"}},{id:"post-gomaxprocs-go\u7684cpu\u6838\u5fc3\u6570\u9650\u5236\u4e0e\u5bb9\u5668\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4f18\u5316",title:"GOMAXPROCS\uff1aGo\u7684CPU\u6838\u5fc3\u6570\u9650\u5236\u4e0e\u5bb9\u5668\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4f18\u5316",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/gomaxprocs-optimizing-go-performance-in-containers/"}},{id:"post-\u5173\u4e8e\u4e2d\u56fd\u9152\u573a\u6587\u5316\u7684\u601d\u8003",title:"\u5173\u4e8e\u4e2d\u56fd\u9152\u573a\u6587\u5316\u7684\u601d\u8003",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/reflections-on-chinas-drinking-culture/"}},{id:"post-go\u8bed\u8a00\u57fa\u4e8ebenchstat\u505a\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u6027\u80fd\u8ddf\u8e2a",title:"Go\u8bed\u8a00\u57fa\u4e8ebenchstat\u505a\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u6027\u80fd\u8ddf\u8e2a",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/benchmarking-n-performance-tracking-in-go-using-benchstat/"}},{id:"post-macos-sequoia-15-0-1-option\u70ed\u952e\u7ed1\u5b9a\u5931\u6548\u95ee\u9898",title:"MacOS Sequoia 15.0.1 Option\u70ed\u952e\u7ed1\u5b9a\u5931\u6548\u95ee\u9898",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/resolving-option-key-hotkey-issues-in-macos-sequoia/"}},{id:"post-\u89c2\u70b9\u8ba4\u540c\u95ee\u9898-the-challenge-of-opinion-shaping-in-the-digital-age",title:"\u89c2\u70b9\u8ba4\u540c\u95ee\u9898 / The Challenge of Opinion Shaping in the Digital Age",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/the-challenge-of-opinion-shaping-in-the-digital-age/"}},{id:"post-\u7528\u79d1\u5b66\u7684\u624b\u6bb5\u91cd\u65b0\u8ba4\u8bc6\u4e60\u4ee5\u4e3a\u5e38\u7684\u4e8b\u7269-reevaluating-the-familiar-through-a-scientific-lens",title:"\u7528\u79d1\u5b66\u7684\u624b\u6bb5\u91cd\u65b0\u8ba4\u8bc6\u4e60\u4ee5\u4e3a\u5e38\u7684\u4e8b\u7269 / Reevaluating the Familiar Through a Scientific Lens",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/reevaluating-the-familiar-through-a-scientific-lens/"}},{id:"post-\u6280\u672f\u9769\u547d\u4e2d\u7684\u535a\u5f08-competing-forces-in-the-tech-revolution",title:"\u6280\u672f\u9769\u547d\u4e2d\u7684\u535a\u5f08 / Competing Forces in the Tech Revolution",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/competing-forces-in-the-tech-revolution/"}},{id:"post-\u76ee\u6807\u5236\u5b9a-\u6807\u51c6\u8861\u91cf\u548c\u4eba\u5fc3\u7ba1\u7406-goal-planning-performance-assessment-and-team-cohesion",title:"\u76ee\u6807\u5236\u5b9a\u3001\u6807\u51c6\u8861\u91cf\u548c\u4eba\u5fc3\u7ba1\u7406 / Goal Planning, Performance Assessment, and Team Cohesion",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/goals-metrics-n-team-spirit/"}},{id:"post-\u6700\u9ad8\u7ea7\u7684\u751f\u6d3b\u65b9\u5f0f-\u5b58\u94b1-\u65e9\u8d77-\u8fd0\u52a8-\u8bfb\u4e66-ultimate-adulting-hack-stashing-cash-racing-the-sunrise-sweating-by-choice-and-pretending-you-actually-finished-that-book",title:"\u6700\u9ad8\u7ea7\u7684\u751f\u6d3b\u65b9\u5f0f\uff1a\u5b58\u94b1\uff0c\u65e9\u8d77\uff0c\u8fd0\u52a8\uff0c\u8bfb\u4e66 / Ultimate Adulting Hack: Stashing Cash, Racing the Sunrise, Sweating by Choice,...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/the-secret-to-peak-living/"}},{id:"post-\u5728\u786e\u5b9a\u6027\u4e2d\u5bfb\u627e\u4e0d\u786e\u5b9a\u6027-seeking-uncertainty-in-certainty",title:"\u5728\u786e\u5b9a\u6027\u4e2d\u5bfb\u627e\u4e0d\u786e\u5b9a\u6027 / Seeking uncertainty in certainty",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/seeking-uncertainty-in-certainty/"}},{id:"post-kafka\u8bbe\u8ba1\u7406\u5ff5-kafka-design-concept",title:"Kafka\u8bbe\u8ba1\u7406\u5ff5 / Kafka Design Concept \ud83d\udcdd",description:"\u4eceKafka Paper\u4e2d\u611f\u53d7\u5176\u8bbe\u8ba1\u601d\u60f3",section:"Posts",handler:()=>{window.location.href="/blog/2024/kafka-design-concept/"}},{id:"post-raft\u8bba\u6587\u9605\u8bfb\u5c0f\u8bb0-raft-paper-reading-notes",title:"Raft\u8bba\u6587\u9605\u8bfb\u5c0f\u8bb0 / RAFT paper reading notes \ud83d\udcdd",description:"\u4eceRaft paper\u4e2d\u611f\u53d7\u5176\u8bbe\u8ba1\u601d\u60f3",section:"Posts",handler:()=>{window.location.href="/blog/2024/raft/"}},{id:"post-zsh-nice-5",title:"zsh nice 5 \ud83e\uddd0",description:"Nice\u503c\u5982\u4f55\u5e72\u6270CPU\u4f7f\u7528\u7387\u7684\u89c2\u6d4b",section:"Posts",handler:()=>{window.location.href="/blog/2024/zsh-nice-5/"}},{id:"news-\u91cd\u65b0\u628a\u6211\u7684blog\u5e26\u56de\u6765\u4e86-\u8ddd\u79bb\u4e0a\u6b21\u5df2\u7ecf\u8fc7\u53bb\u4e8615\u5e74\u4e86-15\u5e74\u524d\u7684\u5144\u5f1f\u4eec-\u65e9\u5df2\u6d88\u5931\u5728\u4e92\u8054\u7f51\u7684\u5404\u4e2a\u89d2\u843d\u91cc\u4e86-\u5e0c\u671b\u4e0b\u4e2a\u5341\u4e94\u5e74\u4e5f\u80fd\u9047\u5230\u5341\u4e94\u5e74\u524d\u7684\u90a3\u4efd\u611f\u52a8\u548c\u5feb\u4e50",title:"\u91cd\u65b0\u628a\u6211\u7684Blog\u5e26\u56de\u6765\u4e86\uff01\u8ddd\u79bb\u4e0a\u6b21\u5df2\u7ecf\u8fc7\u53bb\u4e8615\u5e74\u4e86\uff0c15\u5e74\u524d\u7684\u5144\u5f1f\u4eec\uff0c\u65e9\u5df2\u6d88\u5931\u5728\u4e92\u8054\u7f51\u7684\u5404\u4e2a\u89d2\u843d\u91cc\u4e86\ud83d\ude2d  \u5e0c\u671b\u4e0b\u4e2a\u5341\u4e94\u5e74\u4e5f\u80fd\u9047\u5230\u5341\u4e94\u5e74\u524d\u7684\u90a3\u4efd\u611f\u52a8\u548c\u5feb\u4e50\u2728",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%69%66%75%72%79%73%74@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ifuryst","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/leoifuryst","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@ifuryst.","_blank")}},{id:"socials-stackoverflow",title:"Stackoverflow",section:"Socials",handler:()=>{window.open("https://stackoverflow.com/users/14616272","_blank")}},{id:"socials-unsplash",title:"Unsplash",section:"Socials",handler:()=>{window.open("https://unsplash.com/@ifuryst","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",
section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>